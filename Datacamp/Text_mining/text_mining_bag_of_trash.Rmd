---
title: "Bag of words"
---

```{r eval=FALSE, echo=FALSE}

pacman::p_load(corrplot,
               PerformanceAnalytics,rgdal,ggmap,rgeos,
               revealjs,DT,leaflet,data.table,
               leaflet.extras,caret,rpart,ggplot2,ipred,randomForest,
               rpart.plot,class,rmarkdown,
               BBmisc,ParamHelpers,kernlab,tm, wordcloud)

```

```{r}

#####  chemin relatif du dossier contenant les fichiers ######

data.path="../../../DS/"

######### choix fichier #############

files=list.files(pattern = ".Rmd")
files.excluded = c("102_12_vis_exploration_regression.Rmd", "102_21_regression.rmd", "302_1_regressions_1d.Rmd", "00_regression_preparation.Rmd")

######### type fichier #############

type_document=c("html_document","html")
type_document=c("revealjs::revealjs_presentation","revealjs")
type_document=c("pdf_document","pdf")
type_document=c("beamer_presentation","beamer")

######### render fichiers #############

# les suffixes et préfixes permettent ici, de différencier les sorties en fonction des bases analysées

fprefix = ""
fprefix = "_btp"
fprefix = "_salaire_median"
fprefix = "_agences"
fsuffix = ""

####### liste des options   ############

 ajout.variables = F 
 if( !ajout.variables){
   
   files.excluded = c(files.excluded,"02_regression_ajout_variable.Rmd")
 }

files = setdiff(files, files.excluded)

####### boucle de render  ############

for (file in files){
  render(file,"html_document",
         output_file = paste0(fprefix,substr(file,1,(nchar(file)-4)),fsuffix,".html"),
         encoding="UTF-8",
         output_dir = "html_site",
         quiet = F,
         output_options=list(self_contained=FALSE,
                             lib_dir="html_site/site_libs"))
}

```

```{r echo=FALSE, eval=FALSE}

#####  chemin relatif du dossier contenant les fichiers ######
data.path="https://dataks.bitbucket.io/ml/"
data.path="C:/Users/SHIK/Documents/GitHub/dataks.bitbucket.io/ml/"
data.path="../../../DS/data/test/"
######### choix fichier #############

files=list.files(pattern = ".Rmd")
files.excluded = c("house_price_prediction.Rmd","regression_mlr.Rmd")
files = setdiff(files, files.excluded)

######### type fichier #############

type_document=c("html_document","html")
type_document=c("revealjs::revealjs_presentation","revealjs")
type_document=c("pdf_document","pdf")
type_document=c("beamer_presentation","beamer")

####### boucle de render  ############

for (file in files){
  render(file,"html_document",
         output_file = paste0(substr(file,1,(nchar(file)-4)),".html"),
         encoding="UTF-8",
         output_dir = "html_site",
         quiet = F,
         output_options=list(self_contained=FALSE,
                             lib_dir="html_site/site_libs", toc = TRUE,toc_float = TRUE, number_sections= TRUE))
}

```


# Import data

Nous allons travaillé ici avec le dataset `tweets.

```{r}
data <- read.csv('https://assets.datacamp.com/production/course_935/datasets/coffee.csv', stringsAsFactors = F)
```


# Structure 

On analyse rapidememnt le type de chaque variables explicativs ( à priori toutes sont de type `chatacter`)
```{r}
str(data)
# Isolate text from tweets: coffee_tweets
data.text =  data$text
```

# Corpus 

create a volatile corpus (vcorpus that is held in the ram ) to be more efficient on the processing 
```{r}

# Make a vector source: coffee_source
 data.source <- VectorSource(data.text)
```

```{r}
## coffee_source is already in your workspace


#Call the VCorpus() function on the coffee_source object to create coffee_corpus.
# Make a volatile corpus: coffee_corpus
data.corpus <- VCorpus(data.source)

# Print out coffee_corpus
data.corpus

# Print the 15th tweet in coffee_corpus
data.corpus[[15]]

# Print the contents of the 15th tweet in coffee_corpus

data.corpus[[15]][1]

#Print the content() of the 10th tweet within coffee_corpus
# Now use content to review plain text
content(data.corpus[[10]])

```
Make a VCorpus from a data frame
If your text data is in a data frame you can use DataframeSource() for your analysis. The data frame passed to DataframeSource() must have a specific structure:

Column one must be called doc_id and contain a unique string for each row.
Column two must be called text with "UTF-8" encoding (pretty standard).
Any other columns, 3+ are considered metadata and will be retained as such.


```{r eval = FALSE}

#Create df_source using DataframeSource() with the example_text.
# Create a DataframeSource: df_source
df_source <- DataframeSource(example_text)

#Create df_corpus by converting df_source to a volatile corpus object with VCorpus()
# Convert df_source to a corpus: df_corpus

df_corpus <- VCorpus(df_source)

# Examine df_corpus
df_corpus


# Examine df_corpus metadata
# Use meta() on df_corpus to print the document associated metadata.

meta(df_corpus)

# Compare the number of documents in the vector source
vec_corpus

# Compare metadata in the vector corpus
meta(vec_corpus)

```


# Cleaning and preprocessing 

Il s'agit ici des étapes préliminaires 

- transformer en miniscule
- retirer les signes de   ponctuation

Après ces étapes élémentaires vient le stemming qui consiste à remplacer les mots par leurs racines.

```{r}

```

Après le steming on se retrouve facilement avec des mots qui n'ont pas de sens c'est pourquoi on ajoute une étape 
pour completer les token par les éléments ayant du sens 


```{r eval = FALSE }
# Create the object: text
text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."

# Make lowercase

tolower(text)

# Remove punctuation
removePunctuation(text)

# Remove numbers
removeNumbers(text)

# Remove whitespace
stripWhitespace(text)
```
qdab preprocessing function 
```{r  eval = FALSE }
## text is still loaded in your workspace

# Remove text within brackets
bracketX(text)

# Replace numbers with words
replace_number(text)

# Replace abbreviations
replace_abbreviation(text)

# Replace contractions
replace_contraction(text)

# Replace symbols with words

replace_symbol(text)

```
## Stopword

```{r  eval = FALSE }

## text is preloaded into your workspace

# List standard English stop words

stopwords('en')
# Print text without standard stop words
removeWords(text, stopwords("en"))

# Add "coffee" and "bean" to the list: new_stops
new_stops <- c("coffee", "bean", stopwords('en'))
# Remove stop words from text

removeWords(text, new_stops)
```
# Stemming  

```{r  eval = FALSE }
# Create complicate

complicate <- c('complicated', 'complication', 'complicatedly')

# Perform word stemming: stem_doc
stem_doc <- stemDocument(complicate)

# Create the completion dictionary: comp_dict

comp_dict <- c('complicate')
# Perform stem completion: complete_text 
complete_text <- stemCompletion(stem_doc,comp_dict)

# Print complete_text
complete_text
```

# Word stemming and stem completion on a sentence

```{r eval = FALSE }
comp_dict =c () #  est  la liste qui contient les mots à conserver 
# Remove punctuation: rm_punc
rm_punc <- removePunctuation(text_data)

# Create character vector: n_char_vec
n_char_vec <- unlist(strsplit(rm_punc, split = " "))

# Perform word stemming: stem_doc
stem_doc <- stemDocument(n_char_vec)

# Print stem_doc
stem_doc

# Re-complete stemmed document: complete_doc
complete_doc <- stemCompletion(stem_doc, comp_dict)

# Print complete_doc
complete_doc
```


# Clean corpus function 

```{r}
# Alter the function code to match the instructions
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee", "mug"))
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}

# Create clean_corp by applying clean_corpus() to the included corpus tweet_corp
# Apply your customized function to the tweet_corp: clean_corp

clean.corpus<-  clean_corpus(data.corpus)
```

Print the cleaned 227th tweet in clean_corp using indexing [[227]][1]

```{r}
# Print out a cleaned up tweet
clean.corpus[[227]][1]

# Print out the same tweet in original form
data$text[227]
```

# Make a document-term matrix

```{r}
#Create coffee_dtm by applying DocumentTermMatrix() to clean_corp.
# Create the dtm from the corpus: coffee_dtm
clean.corpus.dtm <- DocumentTermMatrix(clean.corpus)
# Print out coffee_dtm data
clean.corpus.dtm 
# Convert coffee_dtm to a matrix: coffee_m
clean.corpus.m  <- as.matrix(clean.corpus.dtm)
# Print the dimensions of coffee_m
dim(clean.corpus.m)
# Look at documents 475 through 478 and terms 2593 through 2594 (i.e. [475:478, 2593:2594]
# Review a portion of the matrix to get some Starbucks
clean.corpus.m[475:478, 2593:2594]
```

# Common text visualisation 

```{r}
#Create coffee_tdm by applying TermDocumentMatrix() to clean_corp.
# Create a TDM from clean_corp: coffee_tdm
clean.corpus.tdm <- TermDocumentMatrix(clean.corpus)
# Print coffee_tdm data
clean.corpus.tdm
# Convert coffee_tdm to a matrix: coffee_m
clean.corpus.m <- as.matrix(clean.corpus.tdm)
# Print the dimensions of the matrix
dim(clean.corpus.m)
# Review a portion of the matrix
clean.corpus.m[2593:2594, 475:478]
```

#  Frequent terms with tm

```{r}
## coffee_tdm is still loaded in your workspace
# Create a matrix: coffee_m


# Calculate the rowSums: term_frequency
term_frequency <- rowSums(clean.corpus.m)
# Sort term_frequency in descending order
term_frequency = sort(term_frequency, decreasing = TRUE )
# View the top 10 most common words
term_frequency[1:10]
#Make a barplot of the top 10 terms with col = "tan" and las = 2 (for vertical x-axis labels).
# Plot a barchart of the 10 most common words
barplot(term_frequency[1:10], col = "tan" ,las = 2)
```





words clouds

```{r}
## term_frequency is loaded into your workspace


# Print the first 10 entries in term_frequency
term_frequency[1:10]

#Extract the terms using names() on term_frequency. Call the vector of strings terms_vec.
# Vector of terms
terms_vec = names(term_frequency)

# Create a wordcloud for the values in word_freqs

 wordcloud( terms_vec, term_frequency, max.words = 50, colors ='red')

```

# Stop words and word clouds

```{r}
# Review a "cleaned" tweet
content(data.corpus[[24]])

# Add to stopwords
stops <- c(stopwords(kind = 'en'), "chardonnay")

# Review last 6 stopwords 
tail(stops)

# Apply to a corpus
cleaned_chardonnay_corp <- tm_map(data.corpus, removeWords, stops)

# Review a "cleaned" tweet again
content(data.corpus[[24]])
```


```{r}
# Sort the chardonnay_words in descending order

sorted_chardonnay_words = sort(chardonnay_words, decreasing = TRUE)
# Print the 6 most frequent chardonnay terms
sorted_chardonnay_words[1:10]

# Get a terms vector
terms_vec<- names(chardonnay_words)

# Create a wordcloud for the values in word_freqs
wordcloud(chardonnay_words, chardonnay_words, 
          max.words = 50, colors = "red")
```

Improve word cloud colors

```{r}

# Print the list of colors
colors()
# Print the wordcloud with the specified colors
terms <- names(term_frequency)

wordcloud(terms_vec, 
          term_frequency, 
          max.words = 100, 
          colors = c("grey80","darkgoldenrod1", "tomato"))
```

# Use prebuilt color palettes

```{r eval = FALSE}
# Select 5 colors 

color_pal <- cividis(n = 5 )

# Examine the palette output


# Create a wordcloud with the selected palette
wordcloud(terms_vec, 
          term_frequency, max.words = 100, colors = color_pal)
```

# Find common words

```{r}

# Create all_coffee by using paste() with collapse = " " on coffee_tweets$text.
# Create all_coffee

all_coffee = paste(coffee_tweets$text, collapse =" ")
# Create all_chardonnay by using paste() with collapse = " " on chardonnay_tweets$text.
# Create all_chardonnay

all_chardonnay = paste(chardonnay_tweets$text, collapse =" ")
# Create all_tweets using c() to combine all_coffee and all_chardonnay. Make all_coffee the first term.
# Create all_tweets

all_tweets <- c(all_coffee, all_chardonnay)

# Convert to a vector source

all_tweets  <- VectorSource(all_tweets)

# Create all_corpus by using VCorpus() on all_tweets.
# Create all_corpus

all_corpus <- VCorpus(all_tweets)



# Create all_clean by applying the predefined clean_corpus() function to all_corpus.

# Clean the corpus

all_clean <- clean_corpus(all_corpus)

# Create all_tdm, a TermDocumentMatrix from all_clean.

# Create all_tdm
all_tdm <- TermDocumentMatrix(all_clean)

# Create all_m
all_m <- as.matrix(all_tdm)

# Print a commonality cloud

commonality.cloud(all_m, max.words = 100, colors = "springgreen")

```

# Visualize dissimilar words
Say you want to visualize the words not in common. To do this, you can also use comparison.cloud() and the steps are quite similar with one main difference.

Like when you were searching for words in common, you start by unifying the tweets into distinct corpora and combining them into their own VCorpus() object. Next apply a clean_corpus() function and organize it into a TermDocumentMatrix.

To keep track of what words belong to coffee versus chardonnay, you can set the column names of the TDM like this:

```{r}

# Create all_clean by applying the predefined clean_corpus function to all_corpus 
# Clean the corpus
all_clean <- clean_corpus(all_corpus)


# reate all_tdm, a TermDocumentMatrix, from all_clean.
# Create all_tdm
all_tdm <- TermDocumentMatrix(all_clean)
# Give the columns distinct names
colnames(all_tdm) <- c("coffee", "chardonnay")
# Create all_m
all_m <- as.matrix(all_tdm)
# Create comparison cloud
comparison.cloud(all_m, colors = c("orange","blue"), max.words = 50 )
```

# Polarized tag cloud

Commonality clouds show words that are shared across documents. One interesting thing that they can't show you is which of those words appear more commonly in one document compared to another. For this, you need a pyramid plot; these can be generated using pyramid.plot() from the plotrix package.

First, some manipulation is required to get the data in a suitable form. This is most easily done by converting it to a data frame and using dplyr. Given a matrix of word counts, as created by as.matrix(tdm), you need to end up with a data frame with three columns:

The words contained in each document.
The counts of those words from document 1.
The counts of those words from document 2.
Then pyramid.plot() using

```{r}

top25_df <- all_tdm_m %>%
  # Convert to data frame
  as_data_frame(rownames = "word") %>% 
  # Keep rows where word appears everywhere
  filter_all(all_vars(. > 0)) %>% 
  # Get difference in counts
  mutate(difference = chardonnay - coffee) %>% 
  # Keep rows with biggest difference
  top_n(25, wt = difference) %>% 
  # Arrange by descending difference
  arrange(desc(difference))
  
pyramid.plot(
  # Chardonnay counts
  top25_df$chardonnay, 
  # Coffee counts
  top25_df$coffee, 
  # Words
  labels = top25_df$word, 
  top.labels = c("Chardonnay", "Words", "Coffee"), 
  main = "Words in Common", 
  unit = NULL,
  gap = 8,
)


```

# Visualize word networks

Another way to view word connections is to treat them as a network, similar to a social network. Word networks show term association and cohesion. A word of caution: these visuals can become very dense and hard to interpret visually.

In a network graph, the circles are called nodes and represent individual terms, while the lines connecting the circles are called edges and represent the connections between the terms.

For the over-caffeinated text miner, qdap provides a shorcut for making word networks. The word_network_plot() and word_associate() functions both make word networks easy!

The sample code constructs a word network for words associated with "Marvin".
```{r}
# Word association
word_associate(coffee_tweets$text, match.string = "barista" ,
               stopwords = c(Top200Words, "coffee", "amp"), 
               network.plot = TRUE, cloud.colors = c("gray85", "darkred"))

# Add title
title(main = "arista Coffee Tweet Associations")
```

# Teaser: simple word clustering

In the next chapter, we cover some miscellaneous (yet very important) text mining subjects including:

TDM/DTM weighting
Dealing with TDM/DTM sparsity
Capturing metadata
Simple word clustering for topics
Analysis on more than one word
For now, let's simply create a new visual called a dendrogram from our coffee_tweets. The next chapter will explain it in detail.

## Make a dendrogram friendly TDM

```{r}
# Print the dimensions of tweets_tdm

dim(tweets_tdm)
# Create tdm1
tdm1  <- removeSparseTerms(tweets_tdm,sparse = 0.95)

# Create tdm2

tdm2 <- removeSparseTerms(tweets_tdm,sparse = 0.975)
# Print tdm1
tdm1

# Print tdm2
tdm2
```


```{r}
# Create tweets_tdm2

 tweets_tdm2 <- removeSparseTerms(tweets_tdm, sparse = 0.975)
# Create tdm_m
tdm_m <- as.matrix(tweets_tdm2)

# Create tweets_dist

tweets_dist <- dist(tweets_tdm2)
# Create hc
hc =hclust(tweets_dist)


# Plot the dendrogram
plot(hc)
```
# A better dendogramm 

```{r}
#Create hcd as a dendrogram using as.dendrogram() on hc

# Create hcd
hcd = as.dendrogram(hc)

# Print the labels in hcd
hcd

# Change the branch color to red for "marvin" and "gaye"
hcd_colored <- branches_attr_by_labels(hcd, c("marvin", "gaye"), color = 'red')

# Plot hcd
plot(hcd, main = "Better Dendrogram")

# Add cluster rectangles
rect.dendrogram(hcd_colored, k = 2, border = "grey50")
```

## Using word association

Another way to think about word relationships is with the findAssocs() function in the tm package. For any given word, findAssocs() calculates its correlation with every other word in a TDM or DTM. Scores range from 0 to 1. A score of 1 means that two words always appear together in documents, while a score approaching 0 means the terms seldom appear in the same document.

Keep in mind the calculation for findAssocs() is done at the document level. So for every document that contains the word in question, the other terms in those specific documents are associated. Documents without the search term are ignored.

To use findAssocs() pass in a TDM or DTM, the search term, and a minimum correlation. The function will return a list of all other terms that meet or exceed the minimum threshold.
```{r}
#Create associations using findAssocs() on tweets_tdm to find terms associated with "venti", which meet a minimum threshold of 0.2
# Create associations

 associations <- findAssocs(tweets_tdm, "venti", 0.2)
# View the venti associations
associations
#Create associations_df, by calling list_vect2df(), passing associations, then setting col2 to "word" and #col3 to "score"
# Create associations_df
associations_df <- list_vect2df(associations, col2=  "word" , col3 = "score")

# Plot the associations_df values
ggplot(associations_df, aes(score, word)) + 
  geom_point(size = 3) + 
  theme_gdocs()

```

# Changing n-grams
So far, we have only made TDMs and DTMs using single words. The default is to make them with unigrams, but you can also focus on tokens containing two or more words. This can help extract useful phrases which lead to some additional insights or provide improved predictive attributes for a machine learning algorithm.

The function below uses the RWeka package to create trigram (three word) tokens: min and max are both set to 3.

tokenizer <- function(x) {
  NGramTokenizer(x, Weka_control(min = 3, max = 3))
}
Then the customized tokenizer() function can be passed into the TermDocumentMatrix or DocumentTermMatrix functions as an additional parameter:

tdm <- TermDocumentMatrix(
  corpus, 
  control = list(tokenize = tokenizer)
)

```{r}
# Make tokenizer function 
tokenizer <- function(x) {
  NGramTokenizer(x, Weka_control(min = 2, max = 2))
}

# Create unigram_dtm
unigram_dtm <- DocumentTermMatrix(text_corp)

# Create bigram_dtm
bigram_dtm <- DocumentTermMatrix(
  text_corp, 
  control = list(tokenize = tokenizer)
)

# Print unigram_dtm
unigram_dtm

# Print bigram_dtm
bigram_dtm
```

# How do bigrams affect word clouds?

Now that you have made a bigram DTM, you can examine it and remake a word cloud. The new tokenization method affects not only the matrices, but also any visuals or modeling based on the matrices.

Remember how "Marvin" and "Gaye" were separate terms in the chardonnay word cloud? Using bigram tokenization grabs all two word combinations. Observe what happens to the word cloud in this exercise.

This exercise uses str_subset from stringr. Keep in mind, other DataCamp courses cover regular expressions in more detail. As a reminder, the regular expression ^ matches the starting position within the exercise's bigrams.


```{r}
# Create bigram_dtm_m

bigram_dtm_m <- as.matrix(bigram_dtm)
# Create freq
freq <- colSums(bigram_dtm_m)
# Create bi_words
bi_words <- names(freq)
# Examine part of bi_words
str_subset(bi_words,"^marvin")
# Plot a wordcloud
wordcloud(bi_words,freq,max.words = 15)
```


```{r}
# Create a TDM
tdm <- TermDocumentMatrix(text_corp)

# Convert it to a matrix
tdm_m <- as.matrix(tdm)

# Examine part of the matrix
tdm_m[c("coffee", "espresso", "latte"), 161:166 ] 

```

# tidf 

```{r}
# Create a TDM
tdm <- TermDocumentMatrix(text_corp, control = list(weighting = weightTfIdf))

# Convert it to a matrix
tdm_m <- as.matrix(tdm)

# Examine part of the matrix
tdm_m[c("coffee", "espresso", "latte"), 161:166 ] 

```

# Capturing metadata in tm


```{r}
# Rename the first column of tweets to "doc_id".
# Rename columns
names(tweets)[1] <- "doc_id"

# Set the schema: docs
docs <- DataframeSource(tweets)

# Make a clean volatile corpus: text_corpus
text_corpus <- clean_corpus(VCorpus(docs))

# Examine the first doc content
content(text_corpus[[1]])

# Access the first doc metadata
meta(text_corpus[1])
```


# WORFLOW 

The main steps in a text mining project are :

- Problem definition
- unorganized state
- Organisation
- Feature Extraction
- Analysis
- Organized state


# case study 
Exercise
Exercise
Step 2: Identifying the text sources
Employee reviews can come from various sources. If your human resources department had the resources, you could have a third party administer focus groups to interview employees both internally and from your competitor.

Forbes and others publish articles about the "best places to work", which may mention Amazon and Google. Another source of information might be anonymous online reviews from websites like Indeed, Glassdoor or CareerBliss.

Here, we'll focus on a collection of anonymous online revie

```{r}
# View the structure of amzn with str() to get its dimensions and a preview of the data
# Print the structure of amzn
str(amzn)
#reate amzn_cons from the negative reviews column amzn$cons
# Create amzn_pros
amzn_pros <- amzn$pros
# Create amzn_cons
amzn_cons <- amzn$cons

# Print the structure of goog
str(goog)
# Create goog_pros
goog_pros <- goog$pros
# Create goog_cons
goog_cons <- goog$cons
```

# cleaning 

```{r}
# qdap_clean the text

qdap_cleaned_amzn_pros <- qdap_clean(amzn_pros)
# Source and create the corpus
amzn_p_corp <- VCorpus(VectorSource(qdap_cleaned_amzn_pros))

# tm_clean the corpus
amzn_pros_corp <- tm_clean(amzn_p_corp)


```


```{r}
# qdap_clean the text

qdap_cleaned_amzn_cons = qdap_clean(amzn_cons)

# Source and create the corpus
amzn_c_corp = VCorpus(VectorSource(qdap_cleaned_amzn_cons))

# tm_clean the corpus
amzn_cons_corp = tm_clean(amzn_c_corp)

```

```{r}

# qdap_clean the text
qdap_cleaned_goog_pros <- qdap_clean(goog_pros)

# Source and create the corpus
goog_p_corp <- VCorpus(VectorSource(qdap_cleaned_goog_pros))

# tm_clean the corpus
goog_pros_corp <-  tm_clean(goog_p_corp)

```


```{r}
# qdap_clean the text
qdap_cleaned_goog_pros <- qdap_clean(goog_pros)

# Source and create the corpus
goog_p_corp <- VCorpus(VectorSource(qdap_cleaned_goog_pros))

# tm_clean the corpus
goog_pros_corp <-  tm_clean(goog_p_corp)


# qdap_clean the text
qdap_cleaned_goog_cons <- qdap_clean(goog_cons)

# Source and create the corpus
goog_c_corp <- VCorpus(VectorSource(qdap_cleaned_goog_cons ))

# tm_clean the corpus
goog_cons_corp <-  tm_clean(goog_c_corp)

```

Feature extraction & analysis: amzn_pros
amzn_pros_corp, amzn_cons_corp, goog_pros_corp and goog_cons_corp have all been preprocessed, so now you can extract the features you want to examine. Since you are using the bag of words approach, you decide to create a bigram TermDocumentMatrix for Amazon's positive reviews corpus, amzn_pros_corp. From this, you can quickly create a wordcloud() to understand what phrases people positively associate with working at Amazon.

The function below uses RWeka to tokenize two terms and is used behind the scenes in this exercise.

tokenizer <- function(x) {
  NGramTokenizer(x, Weka_control(min = 2, max = 2))
}


```{r}

# Create amzn_p_tdm as a TermDocumentMatrix from amzn_pros_corp. Make sure to add control = list(tokenize = tokenizer) so that the terms are bigrams.
# Create amzn_p_tdm
tokenizer <- function(x) {
  NGramTokenizer(x, Weka_control(min = 2, max = 2))
}
amzn_p_tdm <- TermDocumentMatrix(amzn_pros_corp, control = list(tokenize = tokenizer))

# Create amzn_p_tdm_m
amzn_p_tdm_m <- as.matrix(amzn_p_tdm)

# Create amzn_p_freq to obtain the term frequencies from amzn_p_tdm_m.
amzn_p_freq <- rowSums(amzn_p_tdm_m)
# Create amzn_p_freq


# Plot a wordcloud using amzn_p_freq values

wordcloud(names(amzn_p_freq),amzn_p_freq, max.words = 25 )
```


```{r}
# Create amzn_c_tdm
amzn_c_tdm <- TermDocumentMatrix(amzn_cons_corp
,control = list(tokenize = tokenizer))

# Create amzn_c_tdm_m

amzn_c_tdm_m <- as.matrix(amzn_c_tdm)


# Create amzn_c_freq
amzn_c_freq <- rowSums(amzn_c_tdm_m )

# Plot a wordcloud of negative Amazon bigrams
wordcloud(names(amzn_c_freq), amzn_c_freq,max.words = 25)
```

```{r}
# Create amzn_c_tdm
amzn_c_tdm <- TermDocumentMatrix(amzn_cons_corp, control = list(tokenize = tokenizer))

# Print amzn_c_tdm to the console
amzn_c_tdm

# Create amzn_c_tdm2 by removing sparse terms 
amzn_c_tdm2 <- removeSparseTerms(amzn_c_tdm,.993)

# Create hc as a cluster of distance values
hc <-  hclust(dist(amzn_c_tdm2),method = "complete")

# Produce a plot of hc
plot(hc)

```

# Word association

As expected, you see similar topics throughout the dendrogram. Switching back to positive comments, you decide to examine top phrases that appeared in the word clouds. You hope to find associated terms using the findAssocs()function from tm. You want to check for something surprising now that you have learned of long hours and a lack of work-life balance.

```{r}

# Construct a TDM called amzn_p_tdm from amzn_pros_corp and control = list(tokenize = tokenizer).
# Create amzn_p_tdm

amzn_p_tdm = TermDocumentMatrix(amzn_pros_corp,
                                control = list(tokenize = tokenizer)
)

# Create amzn_p_m

amzn_p_m = as.matrix(amzn_p_tdm)

# Create amzn_p_freq
amzn_p_freq =  rowSums(amzn_p_m)

# Create term_frequency

term_frequency = sort(amzn_p_freq,decreasing = TRUE)

# Print the 5 most common terms

term_frequency[1:5]

# Find associations with fast paced

findAssocs(amzn_p_tdm,"fast paced", 0.2)

```

# Quick review of Google reviews

You decide to create a comparison.cloud() of Google's positive and negative reviews for comparison to Amazon. This will give you a quick understanding of top terms without having to spend as much time as you did examining the Amazon reviews in the previous exercises.

We've provided you with a corpus all_goog_corpus, which has the 500 positive and 500 negative reviews for Google. Here, you'll clean the corpus and create a comparison cloud comparing the common words in both pro and con reviews.


```{r}
# Create all_goog_corp
all_goog_corp <- tm_clean(all_goog_corpus)

# Create all_tdm

all_tdm <- TermDocumentMatrix(all_goog_corp)

# Create all_m
all_m <- as.matrix(all_tdm)

# Build a comparison cloud
comparison.cloud(all_m , 
    max.words = 100, 
    colors = c("#F44336", "#2196f3"))

```

```{r}
# Filter to words in common and create an absolute diff column
common_words <- all_tdm_df %>% 
  filter(
    AmazonPro != 0,
    GooglePro != 0
  ) %>%
  mutate(diff = abs(AmazonPro - GooglePro))

# Extract top 5 common bigrams
( top5_df <- top_n(common_words,5, diff))

# Create the pyramid plot
pyramid.plot(top5_df$AmazonPro , top5_df$GooglePro, 
             labels = top5_df$terms, gap = 12, 
             top.labels = c("Amzn", "Pro Words", "Goog"), 
             main = "Words in Common", unit = NULL)

```

Cage match, part 2! Negative reviews
In both organizations, people mentioned "culture" and "smart people", so there are some similar positive aspects between the two companies. However with the pyramid plot you can start to infer degrees of positive features of the work environments.

You now decide to turn your attention to negative reviews and make the same visual. This time you already have the common_words data frame in your workspace. However, the common bigrams in this exercise come form negative employee reviews.

```{r}
# Extract top 5 common bigrams
(top5_df <- top_n(common_words, 5 , diff))

# Create a pyramid plot
pyramid.plot(
    # Amazon on the left
    top5_df$AmazonNeg,
    # Google on the right
    top5_df$GoogleNeg,
    # Use terms for labels
    labels = top5_df$terms,
    # Set the gap to 12
    gap = 12,
    # Set top.labels to "Amzn", "Neg words" & "Goog"
   top.labels = c("Amzn", "Neg Words", "Goog"),
    main = "Words in Common", 
    unit = NULL
)
```

Draw another conclusion, insight, or recommendation
Earlier you were surprised to see "fast paced" in the pros despite the other reviews mentioning "work-life balance". Recall that you used findAssocs() to get a named vector of phrases. These may lead you to a conclusion about the type of person who favorably views an intense workload.

Given the abbreviated results of the associated phrases, what would you recommend Amazon HR recruiters look for in candidates? (You can use the snippet below to gain insight on phrases associated with "fast paced".)

findAssocs(amzn_p_tdm, "fast paced", 0.2)[[1]][1:15]


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```


```{r}

```


```{r}

```

```{r}

```

```{r}

```


```{r}

```

```{r}

```
---
title: "inference for linear regression "
---

```{r eval=FALSE, echo=FALSE}
pacman::p_load(rgeos,rgdal,learnr,skimr,ggvis,googleVis,corrplot,
               PerformanceAnalytics,rgdal,ggmap,rgeos,
               revealjs,DT,leaflet,data.table,
               leaflet.extras,caret,rpart,ggplot2,ipred,randomForest,
               rpart.plot,class,e1071,neuralnet,nnet,devtools,
               xgboost,AUC,hydroGOF,gbm,parallel,readr,RColorBrewer,gridExtra,
               rpart,psych,rmarkdown, rpart.plot,corrplot,DT,
               BBmisc,ParamHelpers,kernlab,ranger,irace,cmaes,
               GenSA,XML,mlr,MASS,arules,
               arulesViz,sjPlot,sjmisc,pROC,readxl,tidyverse,broom,staplr,Hmisc,
               tabplot,GGally,insuranceData,dummies,fastDummies,glmnet,plotly,dplyr
               ,DiagrammeR,Ckmeans.1d.dp,caret,lubridate, magrittr,knitr, ggpubr,countrycode,pROC,verification,countrycode,shinydashboard,shiny)
```

```{r echo=FALSE, eval=FALSE}


#####  chemin relatif du dossier contenant les fichiers ######

data.path="https://dataks.bitbucket.io/ml/"
data.path="C:/Users/SHIK/Documents/GitHub/dataks.bitbucket.io/ml/"
data.path="../../../DS/data/test/"

######### choix fichier #############

files=list.files(pattern = ".Rmd")
files.excluded = c("house_price_prediction.Rmd","regression_mlr.Rmd")
files = setdiff(files, files.excluded)

######### type fichier #############

type_document=c("html_document","html")
type_document=c("revealjs::revealjs_presentation","revealjs")
type_document=c("pdf_document","pdf")
type_document=c("beamer_presentation","beamer")

####### boucle de render  ############

for (file in files){
  render(file,"html_document",
         output_file = paste0(substr(file,1,(nchar(file)-4)),".html"),
         encoding="UTF-8",
         output_dir = "html_site",
         quiet = F,
         output_options=list(self_contained=FALSE,
                             lib_dir="html_site/site_libs", toc = TRUE,toc_float = TRUE, number_sections= TRUE))
}

```

# Regression output: example I

The following code provides two equivalent methods for calculating the most important pieces of the linear model output. Recall that the p-value is the probability of the observed data (or more extreme) given the null hypothesis is true. As with inference in other settings, you will need the sampling distribution for the statistic (here the slope) assuming the null hypothesis is true. You will generate the null sampling distribution in later chapters, but for now, assume that the null sampling distribution is correct. Additionally, notice that the standard error of the slope and intercept estimates describe the variability of those estimates.
```{r}

# Load the mosaicData package and the RailTrail data
library(mosaicData)
data(RailTrail)

# Fit a linear model
ride_lm <- lm(volume~hightemp, data = RailTrail)

# View the summary of your model
summary(ride_lm)

# Print the tidy model output
tidy(ride_lm)

```

# First random sample, second random sample

Now, you will dive in to understanding how linear models vary from sample to sample. Here two random samples from a population are plotted onto the scatterplot. The population data (called popdata) already exists and is pre-loaded, along with ggplot and dplyr.

```{r}
# Using popdata, plot response vs. explanatory
# Using popdata, plot response vs. explanatory
ggplot(popdata, aes(x = explanatory , y = response)) + 
  # Add a point layer
  geom_point() + 
  # Add a smooth trend layer, using lin. reg., no ribbon
  geom_smooth(method = "lm", se = FALSE) 
```


```{r}
# From previous step
set.seed(4747)
both_samples <- bind_rows(
  popdata %>% sample_n(size = 50), 
  popdata %>% sample_n(size = 50), 
  .id = "replicate"
)

# Using both_samples, plot response vs. explanatory, colored by replicate
ggplot(both_samples, aes(x = explanatory, y =  response, color = replicate)) +
geom_point()+
  # Add a point layer
geom_smooth(method ="lm", se = FALSE)
  # Add a smooth trend layer, using lin. reg., no ribbon
  

```


# Superimpose lines
Building on the previous exercise, you will now repeat the sampling process 100 times in order to visualize the sampling distribution of regression lines generated by 100 different random samples of the population.

Rather than repeatedly calling sample_n(), like you did in the previous exercise, rep_sample_n() from the oilabs package provides a convenient way to generate many random samples. The function rep_sample_n() repeats the sample_n() command reps times.

The function do() from dplyr will allow you to run the lm call separately for each level of a variable that has been group_by'ed. Here, the group variable is the sampling replicate, so each lm is run on a different random sample of the data.

```{r}
# From previous step
set.seed(4747)
many_samples <- popdata %>% rep_sample_n(size = 50, reps = 100)

# Using many_samples, plot response vs. explanatory, grouped by replicate
ggplot(many_samples, aes(x = explanatory, y = response, group = replicate)) + 
  # Add a point layer
  geom_point() + 
  # Add a smooth trend line, using lin. reg., no ribbon
  geom_smooth(method = "lm", se = FALSE) 
```

```{r}
# From previous step
set.seed(4747)
many_samples <- popdata %>% rep_sample_n(size = 50, reps = 100)

many_lms <- many_samples %>% 
  # Group by replicate
  group_by(replicate) %>% 
  # Run the model on each replicate, then tidy it
  do(lm(response ~ explanatory, data = .) %>% tidy()) %>%
  # Filter for rows where the term is explanatory
  filter(term == "explanatory")

# See the result
many_lms
```



```{r}
# From previous steps
set.seed(4747)
many_samples <- popdata %>% rep_sample_n(size = 50, reps = 100)
many_lms <- many_samples %>% 
  group_by(replicate) %>% 
  do(lm(response ~ explanatory, data=.) %>% tidy()) %>%
  filter(term == "explanatory")

# Using many_lms, plot estimate
ggplot(many_lms, aes(x = estimate)) +
  # Add a histogram layer
  geom_histogram()
```

```{r}

```